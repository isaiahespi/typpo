@article{fillmore-patrick2025,
  title = {The {{Logit Model Measurement Problem}}},
  author = {Fillmore-Patrick, Stella},
  date = {2025-04},
  journaltitle = {Philosophy of Science},
  volume = {92},
  number = {2},
  pages = {285--303},
  issn = {0031-8248, 1539-767X},
  doi = {10.1017/psa.2024.25},
  abstract = {Traditional wisdom dictates that statistical model outputs are estimates, not measurements. Despite this, statistical models are employed as measurement instruments in the social sciences. In this article, I scrutinize the use of a specific model---the logit model---for psychological measurement. Given the adoption of a criterion for measurement that I call comparability, I show that the logit model fails to yield measurements due to properties that follow from its fixed residual variance.},
  langid = {english},
  file = {D:\ZotMoov\Fillmore-Patrick - 2025 - The Logit Model Measurement Problem.pdf}
}

@article{hauser2018,
  title = {Are {{Manipulation Checks Necessary}}?},
  author = {Hauser, David J. and Ellsworth, Phoebe C. and Gonzalez, Richard},
  date = {2018-06-21},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {9},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.00998},
  abstract = {Researchers are concerned about whether manipulations have the intended effects. Many journals and reviewers view manipulation checks favorably, and they are widely reported in prestigious journals. However, the prototypical manipulation check is a verbal (rather than behavioral) measure that always appears at the same point in the procedure (rather than its order being varied to assess order effects). Embedding such manipulation checks within an experiment comes with problems. While we conceptualize manipulation checks as measures, they can also act as interventions which initiate new processes that would otherwise not occur. The default assumption that manipulation checks do not affect experimental conclusions is unwarranted. They may amplify, undo, or interact with the effects of a manipulation. Further, the use of manipulation checks in mediational analyses does not rule out confounding variables, as any unmeasured variables that correlate with the manipulation check may still drive the relationship. Alternatives such as nonverbal and behavioral measures as manipulation checks and pilot testing are less problematic. Reviewers should view manipulation checks more critically, and authors should explore alternative methods to ensure the effectiveness of manipulations.},
  langid = {english},
  file = {D:\ZotMoov\Hauser et al. - 2018 - Are Manipulation Checks Necessary.pdf}
}

@article{kane2019,
  title = {No {{Harm}} in {{Checking}}: {{Using Factual Manipulation Checks}} to {{Assess Attentiveness}} in {{Experiments}}},
  shorttitle = {No {{Harm}} in {{Checking}}},
  author = {Kane, John V. and Barabas, Jason},
  date = {2019},
  journaltitle = {American Journal of Political Science},
  volume = {63},
  number = {1},
  pages = {234--249},
  issn = {1540-5907},
  doi = {10.1111/ajps.12396},
  abstract = {Manipulation checks are often advisable in experimental studies, yet they rarely appear in practice. This lack of usage may stem from fears of distorting treatment effects and uncertainty regarding which type to use (e.g., instructional manipulation checks [IMCs] or assessments of whether stimuli alter a latent independent variable of interest). Here, we first categorize the main variants and argue that factual manipulation checks (FMCs)---that is, objective questions about key elements of the experiment---can identify individual-level attentiveness to experimental information and, as a consequence, better enable researchers to diagnose experimental findings. We then find, through four replication studies, little evidence that FMC placement affects treatment effects, and that placing FMCs immediately post-outcome does not attenuate FMC passage rates. Additionally, FMC and IMC passage rates are only weakly related, suggesting that each technique identifies different sets of attentive subjects. Thus, unlike other methods, FMCs can confirm attentiveness to experimental protocols.},
  langid = {english},
  file = {C:\Users\isaia\Zotero\storage\G7JFZ8YN\ajps.html}
}

@article{kane2023,
  title = {Analyze the Attentive and Bypass Bias: Mock Vignette Checks in Survey Experiments},
  shorttitle = {Analyze the Attentive and Bypass Bias},
  author = {Kane, John V. and Velez, Yamil R. and Barabas, Jason},
  date = {2023-04},
  journaltitle = {Political Science Research and Methods},
  volume = {11},
  number = {2},
  pages = {293--310},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2023.3},
  abstract = {Respondent inattentiveness threatens to undermine causal inferences in survey-based experiments. Unfortunately, existing attention checks may induce bias while diagnosing potential problems. As an alternative, we propose ``mock vignette checks'' (MVCs), which are objective questions that follow short policy-related passages. Importantly, all subjects view the same vignette before the focal experiment, resulting in a common set of pre-treatment attentiveness measures. Thus, interacting MVCs with treatment indicators permits unbiased hypothesis tests despite substantial inattentiveness. In replications of several experiments with national samples, we find that MVC performance is significantly predictive of stronger treatment effects, and slightly outperforms rival measures of attentiveness, without significantly altering treatment effects. Finally, the MVCs tested here are reliable, interchangeable, and largely uncorrelated with political and socio-demographic variables.},
  langid = {english}
}

@article{marquez2025,
  title = {Ancient {{Tyranny}} and {{Modern Dictatorship}}},
  author = {M\'arquez, Xavier},
  date = {2025-01},
  journaltitle = {The Review of Politics},
  volume = {87},
  number = {1},
  pages = {67--92},
  issn = {0034-6705, 1748-6858},
  doi = {10.1017/S0034670524000445},
  abstract = {This article traces the conceptual history of key terms used to describe and criticize bad political regimes, focusing on the displacement of ``tyranny'' by ``dictatorship'' and ``authoritarianism.'' Classical Greek thought understood tyranny primarily in terms of the character of rulers, whereas the modern idea of dictatorship emerged from a Roman conceptual framework that focused on authority and its legitimation. New problems of legitimation in the eighteenth and nineteenth centuries diminished the utility of the character-centric concept of tyranny and increased the fruitfulness of dictatorship for political analysis. The emergence of the modern state in the nineteenth century shaped the conceptual field by increasing the salience of problems concerning the appropriation or usurpation of sovereignty, the distortion of popular legitimation and accountability, and the incentives for submission to illegitimate orders. I conclude that the use of ``authoritarianism'' is likely to increase in prominence, but that retaining multiple regime concepts enriches analysis.},
  langid = {english},
  file = {D:\ZotMoov\MÃ¡rquez - 2025 - Ancient Tyranny and Modern Dictatorship.pdf}
}

@article{varaine2023,
  title = {How {{Dropping Subjects Who Failed Manipulation Checks Can Bias Your Results}}: {{An Illustrative Case}}},
  shorttitle = {How {{Dropping Subjects Who Failed Manipulation Checks Can Bias Your Results}}},
  author = {Varaine, Simon},
  date = {2023-07},
  journaltitle = {Journal of Experimental Political Science},
  volume = {10},
  number = {2},
  pages = {299--305},
  issn = {2052-2630, 2052-2649},
  doi = {10.1017/XPS.2022.28},
  abstract = {Manipulations checks are postexperimental measures widely used to verify that subjects understood the treatment. Some researchers drop subjects who failed manipulation checks in order to limit the analyses to attentive subjects. This short report offers a novel illustration on how this practice may bias experimental results: in the present case, through confirming a hypothesis that is likely false. In a survey experiment, subjects were primed with a fictional news story depicting an economic decline versus prosperity. Subjects were then asked whether the news story depicted an economic decline or prosperity. Results indicate that responses to this manipulation check captured subjects' preexisting beliefs about the economic situation. As a consequence, dropping subjects who failed the manipulation check mixes the effects of preexisting and induced beliefs, increasing the risk of false positive findings. Researchers should avoid dropping subjects based on posttreatment measures and rely on pretreatment measures of attentiveness.},
  langid = {english}
}
